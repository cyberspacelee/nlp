{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 什么是特征选择\n",
    "特征选择对于训练模型来说非常重要，好的特征选择能够提升模型的性能，更能帮助我们理解数据的特点、底层结构，这对进一步改善模型、算法都有着重要作用。\n",
    "\n",
    "特征选择主要有两个功能：\n",
    "- 减少特征数量、降维，使模型泛化能力更强，减少过拟合\n",
    "- 增强对特征和特征值之间的理解\n",
    "\n",
    "**常用的特征选择方法：**\n",
    "- 去除低方差的特征\n",
    "- 单变量特征选择（卡方和互信息）\n",
    "- 基于模型的特征选择（L1正则模型，树模型）\n",
    "\n",
    "## 2. 去除低方差的特征\n",
    "假设某特征的特征值只有0和1，并且在所有输入样本中，95%的样本的该特征取值都是1，那么可以认为0这个特征作用不大。如果100%都是1，特征0就没意义了。\n",
    "因此必须去除方差为0的特征，并根据需求去除小方差特征。\n",
    "\n",
    "注：此方法仅适用于离散样本\n",
    "### 2.1 API\n",
    "sklearn.feature_selection.VarianceThreshold(threshold=0.0)\n",
    "\n",
    "**常用参数：**\n",
    "- threshold：训练集方差低于这个阈值的特征将被删除。默认情况是保留所有方差不为零的特征，即删除所有样本中数值相同的特征。\n",
    "\n",
    "**属性：**\n",
    "- variances_：特征方差\n",
    "\n",
    "**方法：**\n",
    "- fit(X[, y])：从X中训练经验方差\n",
    "- fit_transform(X[, y])：对数据进行拟合，然后进行转换\n",
    "- transform(X)：减少X的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0],\n",
       "       [1, 4],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "X = [[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]]\n",
    "selector = VarianceThreshold()\n",
    "selector.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.22222222, 2.88888889, 0.        ])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.variances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [4],\n",
       "       [1]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = VarianceThreshold(1)\n",
    "selector.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 单变量特征选择\n",
    "### 3.1 卡方选择\n",
    "卡方选择是专门针对离散型标签（即分类问题）的相关性过滤。通过计算每个非负特征和标签之间的卡方统计量，并依照卡方统计量由高到低为特征排名，选出前K个分数最高的特征的类。\n",
    "\n",
    "公式：\n",
    "$$\\chi2=\\sum^n_{i=1}\\frac{(A−T)^2}{T}$$\n",
    "\n",
    "其中 $A$ 为实际观测值，$T$ 为理论推断值。从上式可以很清晰地看到，计算得到的 $\\chi2$ 值越大，相关性越高。\n",
    "\n",
    "\n",
    "|      | 肠癌患者 | 非肠癌患者 | 总计 |\n",
    "| :--- | :------- | :--------- | :--- |\n",
    "| 穷人 | 67       | 83         | 150  |\n",
    "| 富人 | 102      | 64         | 166  |\n",
    "| 总计 | 169      | 147        | 316  |\n",
    "\n",
    "在上表中四个频数就是卡方检验公式里的 $A$ ，即 $A=[67, 83, 102, 64]$。\n",
    "\n",
    "在这个数据中，肠癌患者约占总患者人数的53.48%。如果我们假设患者的收入水平与其患肠癌无关，那理论上，表格应该如下：\n",
    "\n",
    "|      | 肠癌患者         | 非肠癌患者       | 总计 |\n",
    "| :--- | :--------------- | :--------------- | :--- |\n",
    "| 穷人 | 150x53.48% $\\approx$ 80 | 150x46.52% $\\approx$ 70 | 150  |\n",
    "| 富人 | 166x53.48% $\\approx$ 89 | 166x46.52% $\\approx$ 77 | 166  |\n",
    "| 总计 | 169              | 147              | 316  |\n",
    "\n",
    "理论推断值 $T=[80, 70, 89, 77]$。\n",
    "\n",
    "代入卡方检验公式得：\n",
    "$$\\chi2=(67−80)^2 / 80+(83−70)^2 / 70+(102−89)^2 / 89+(64−77)^2 / 77 \\approx 8.62$$\n",
    "\n",
    "计算得到了 $\\chi2$ 值，通过查卡方分布临界值表判断“患者的收入水平与其患肠癌无关”的假设是否成立，根据 $\\chi2$ 值和其对应的自由度 $自由度n=(行数 - 1) \\times (列数 - 1)$。例子中，8.62 > 6.64, 就认为有1%的概率认为“患者的收入水平与其患肠癌无关”的假设成立，即有99%的置信度认为“患者的收入水平与其患肠癌有关”。\n",
    "![](img/chi2.png)\n",
    "\n",
    "#### 3.1.1 API\n",
    "sklearn.feature_selection.chi2(X, y)\n",
    "\n",
    "**常用参数：**\n",
    "- X：特征矩阵\n",
    "- y：目标向量（类标签）\n",
    "\n",
    "**返回值：**\n",
    "- chi2：array，每个特征的chi2统计信息\n",
    "- pval：array，每个特征的p值\n",
    "\n",
    "常和 sklearn.feature_selection.SelectKBest(score_func=<function f_classif>, *, k=10) 组合，选出前 K 个分数最高的特征。\n",
    "\n",
    "**常用参数：**\n",
    "- score_func：处理函数，如chi2\n",
    "- k：要保留的前 K 个分数最高的特征\n",
    "\n",
    "**属性：**\n",
    "- scores_：特征的分数\n",
    "- pvalues_：特征分数的p值，如果score_func只返回分数，则为None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "X, y = load_digits(return_X_y=True)\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 20)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = SelectKBest(chi2, k=20).fit_transform(X, y)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 互信息\n",
    "互信息法是用来捕捉每个特征与标签之间的任意关系（包括线性和非线性关系）的过滤方法。既可以做回归也可以做分类，互信息法可以找出任意关系。\n",
    "\n",
    "互信息法回“每个特征与目标之间的互信息量的估计”，这个估计量在[0,1]之间，取值为0则表示两个变量独立，为1则表示两个变量完全相关。\n",
    "#### 3.2.1 API\n",
    "回归：sklearn.feature_selection.mutual_info_regression(X, y, *, discrete_features='auto', n_neighbors=3, copy=True, random_state=None)\n",
    "\n",
    "分类：sklearn.feature_selection.mutual_info_classif(X, y, discrete_features='auto', n_neighbors=3, copy=True, random_state=None)\n",
    "\n",
    "**常用参数：**\n",
    "- X：特征矩阵。\n",
    "- y：标签向量。\n",
    "- discrete_features：如果为'auto'，则将其分配给False（表示稠密）X，将其分配给True（表示稀疏）X，如果是bool，则确定是考虑所有特征是离散特征还是连续特征。，如果是数组，则它应该是具有形状（n_features，）的布尔蒙版或具有离散特征索引的数组。\n",
    "- n_neighbors：用于连续变量的MI估计的邻居数;较高的值会减少估计的方差，但可能会带来偏差。\n",
    "- copy: bool, default=True，是否复制给定的数据。如果设置为False，则初始数据将被覆盖。\n",
    "\n",
    "**返回值：**\n",
    "- mi: ndarray，每个特征和标签之间的估计相互信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 基于模型的特征选择\n",
    "### 4.1 L1正则模型\n",
    "L1 正则化将系数 $w$ 的l1范数作为惩罚项加到损失函数上，由于正则项非零，这就迫使那些弱的特征所对应的系数变成0。因此L1正则化往往会使学到的模型很稀疏（系数$w$经常为0），这个特性使得L1正则化成为一种很好的特征选择方法。\n",
    "#### 4.1.1 API\n",
    "sklearn.linear_model.Lasso(alpha=1.0, *, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
    "\n",
    "**参数：**\n",
    "- alpha：$\\alpha$ 值，$\\alpha$ 是一个可调的参数，控制着正则化的强度。\n",
    "- fit_intercept：一个布尔值，制定是否需要b值。\n",
    "- max_iter：一个整数，指定最大迭代数。\n",
    "- normalize：一个布尔值。如果为True，那么训练样本会在回归之前会被归一化。\n",
    "- copy_X：一个布尔值。如果为True，会复制X，否则会覆盖X。\n",
    "- tol：一个浮点数，指定判断迭代收敛与否的一个阈值。\n",
    "- warm_start：一个布尔值。如果为True，那么使用前一次训练结果继续训练，否则从头开始训练。\n",
    "- positive：一个布尔值。如果为True，那么强制要求权重向量的分量都为整数。\n",
    "- selection：一个字符串，可以选择‘cyclic’或者‘random’。它指定了当每轮迭代的时候，选择权重向量的哪个分量来更新。\n",
    " - ‘ramdom’：更新的时候，随机选择权重向量的一个分量来更新。\n",
    " - ‘cyclic’：更新的时候，从前向后一次选择权重向量的一个分量来更新。\n",
    "- random_state：一个整数或者一个RandomState实例，或者None。如果为整数，则它指定了随机数生成器的种子。如果为RandomState实例，则指定了随机数生成器。如果为None，则使用默认的随机数生成器。 \n",
    "\n",
    "**返回值：**\n",
    "- coef_：权重向量。\n",
    "- intercept：b值。\n",
    "- n_iter_：实际迭代次数。\n",
    "\n",
    "**方法：**\n",
    "- fix(X,y[,sample_weight])： 训练模型。\n",
    "- predict(X)：用模型进行预测，返回预测值。\n",
    "- score(X,y[,sample_weight])：返回预测性能的得分，不大于1，越大效果越好。\n",
    "- get_params（[deep]）：获取此估计器的参数。\n",
    "- set_params（** params）： 设置此估计器的参数。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.85, 0.  ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.Lasso(alpha=0.1)\n",
    "clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n",
    "\n",
    "clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15000000000000002"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 树模型\n",
    "随机森林具有准确率高、鲁棒性好、易于使用等优点，这使得它成为了目前最流行的机器学习算法之一。随机森林提供了两种特征选择的方法：平均不纯度减少(mean decrease impurity) 和 mean decrease accuracy。\n",
    "\n",
    "#### 4.2.1 平均不纯度减少 (Mean Decrease Impurity)\n",
    "随机森林由多个决策树构成。决策树中的每一个节点都是关于某个特征的条件，为的是将数据集按照不同的响应变量一分为二。利用不纯度可以确定节点（最优条件），对于分类问题，通常采用 基尼不纯度 或者 信息增益 ，对于回归问题，通常采用的是 方差 或者 最小二乘拟合。当训练决策树的时候，可以计算出每个特征减少了多少树的不纯度。对于一个决策树森林来说，可以算出每个特征平均减少了多少不纯度，并把它平均减少的不纯度作为特征选择的值。\n",
    "\n",
    "#### 4.2.2 平均精确率减少 (Mean Decrease Accuracy)\n",
    "另一种常用的特征选择方法就是直接度量每个特征对模型精确率的影响。主要思路是打乱每个特征的特征值顺序，并且度量顺序变动对模型的精确率的影响。很明显，对于不重要的变量来说，打乱顺序对模型的精确率影响不会太大，但是对于重要的变量来说，打乱顺序就会降低模型的精确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
