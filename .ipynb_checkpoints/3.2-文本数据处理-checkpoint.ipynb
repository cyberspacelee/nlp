{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习算法不能直接处理原始文本，文本必须转换成数字。具体来说，是数字的向量。\n",
    "## 1. 词袋法-BOW(Bag-of-words)\n",
    "### 1.1 什么是词袋模型？\n",
    "词袋模型是 n-gram 语法模型的特例一元模型。\n",
    "该模型忽略掉文本的语法和语序等要素，将其仅仅看作是若干个词汇的集合，文档中每个单词的出现都是独立的。\n",
    "将所有词语装进一个袋子里，不考虑其词法和语序的问题，即每个词语都是独立的。\n",
    "例如例句：\n",
    "1. Bob works in Beijing. \n",
    "2. Jane works in Shanghai.\n",
    "\n",
    "就可以构成一个词袋，袋子里包括Bob、works、in、Beijing、Jane、Shanghai。\n",
    "\n",
    "假设建立一个数组（或词典）用于映射匹配"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Bob, works, in, Beijing, Jane, Shanghai]\n",
    "\n",
    "那么上面两个例句就可以用以下向量表示，对应的下标与映射数组的下标相匹配，其值为该词语出现的次数\n",
    "\n",
    "[1, 1, 1, 1, 0, 0]\n",
    "\n",
    "[0, 1, 1, 0, 1, 1]\n",
    "\n",
    "这两个词频向量就是词袋模型，可以很明显的看到语序关系已经完全丢失。\n",
    "\n",
    "### 1.2 API\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "**参数：**\n",
    "CountVectorizer(*, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
    "\n",
    "CountVectorizer类的参数很多，分为三个处理步骤：preprocessing、tokenizing、n-grams generation.\n",
    "\n",
    "一般要设置的参数是:ngram_range,max_df，min_df，max_features等\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gram_range：例如ngram_range(min,max)，是指将text分成min，min+1，min+2,.........max 个不同的词组。比如 '我 爱 中国' 中ngram_range(1,3)之后可得到'我'  '爱'  '中国'  '我 爱'  '爱 中国' 和'我 爱 中国'，如果是ngram_range (1,1) 则只能得到单个单词'我'  '爱'和'中国'。\n",
    "- max_df：可以设置为范围在[0.0 1.0]的float，也可以设置为没有范围限制的int，默认为1.0。这个参数的作用是作为一个阈值，当构造语料库的关键词集的时候，如果某个词的document frequence大于max_df，这个词不会被当作关键词。如果这个参数是float，则表示词出现的次数与语料库文档数的百分比，如果是int，则表示词出现的次数。如果参数中已经给定了vocabulary，则这个参数无效。\n",
    "- min_df：类似于max_df，不同之处在于如果某个词的document frequence小于min_df，则这个词不会被当作关键词。\n",
    "- max_features：默认为None，可设为int，对所有关键词的term frequency进行降序排序，只取前max_features个作为关键词集。\n",
    "- analyzer：一般使用默认，可设置为string类型，如’word’, ‘char’, ‘char_wb’，还可设置为callable类型，比如函数是一个callable类型。\n",
    "- stop_words：设置停用词，设为english将使用内置的英语停用词，设为一个list可自定义停用词，设为None不使用停用词，设为None且 $max\\_df \\in [0.7, 1.0)$ 将自动根据当前的语料库建立停用词表。\n",
    "- token_pattern：过滤规则，表示token的正则表达式，需要设置analyzer == ‘word’，默认的正则表达式选择2个及以上的字母或数字作为token，标点符号默认当作token分隔符，而不会被当作token。\n",
    "- binary：默认为False，一个关键词在一篇文档中可能出现n次，如果binary=True，非零的n将全部置为1，这对需要布尔值输入的离散概率模型的有用的。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 属性和方法\n",
    "**属性：**\n",
    "- vocabulary_：词汇表；字典型\n",
    "- stop_words_：返回停用词表\n",
    "\n",
    "**方法：**\n",
    "- fit_transform(X)：拟合模型，并返回文本矩阵\n",
    "- fit(raw_documents[, y])：Learn a vocabulary dictionary of all tokens in the raw documents.\n",
    "- fit_transform(raw_documents[, y])：Learn the vocabulary dictionary and return term-document matrix.\n",
    "- get_feature_names()：所有文本的词汇；列表型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = vectorizer.fit_transform(corpus)\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<4x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 21 stored elements in Compressed Sparse Row format>,\n",
       " array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "        [0, 2, 0, 1, 0, 1, 1, 0, 1],\n",
       "        [1, 0, 0, 1, 1, 0, 1, 1, 1],\n",
       "        [0, 1, 1, 1, 0, 0, 1, 0, 1]], dtype=int64))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and this',\n",
       " 'document is',\n",
       " 'first document',\n",
       " 'is the',\n",
       " 'is this',\n",
       " 'second document',\n",
       " 'the first',\n",
       " 'the second',\n",
       " 'the third',\n",
       " 'third one',\n",
       " 'this document',\n",
       " 'this is',\n",
       " 'this the']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "X2 = vectorizer2.fit_transform(corpus)\n",
    "vectorizer2.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TF-IDF\n",
    "### 2.1 什么是TF-IDF\n",
    "除了Bow之外，还有另一种称为TF-IDF的方法。这是TF和IDF的组合，简而言之，它意味着“评估句子中单词的重要性”。\n",
    "$$ TF-IDF = TF \\cdot IDF $$\n",
    "**TF**：频率(term frequency)。它显示了每个句子中单词出现的频率，频率越高，就越重要。\n",
    "\n",
    "公式如下：\n",
    "$$ TF = \\frac {n}{N} $$\n",
    "$n$ 表示某个词在文档中出现的次数，$N$ 表示文档中所有词出现的次数总和，这是一个归一化的过程，目的是消除文档篇幅长短上的差异。\n",
    "\n",
    "**IDF**：逆文档频率(inverse document frequency)，某个词的idf值计算公式如下：\n",
    "$$ IDF = log[\\frac{D}{1+d}]$$\n",
    "$D$ 表示语料中所有的文档总数，$d$ 表示语料中出现某个词的文档数量，公式中的 $1$ 是为了防止分母为 $0$ 的情况，$log$ 是以 $10$ 为底的对数，具有类似于增强区分度的作用（拥挤的值尽可能散开，离群的值尽可能合拢）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 API\n",
    "TfidfVectorizer(*, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, analyzer='word', stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.float64'>, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)[source]\n",
    "\n",
    "**常用的参数：**\n",
    "- encoding：编码格式，默认是 utf-8\n",
    "- ngram_range：N元Gram，元组形式 tuple (min_n, max_n)，表示最后得到的特征可以由几个单部分（词/句子等）构成，min_n <= n <= max_n，例如(1,2)表示，得到的特征可以由1个或者2个连续的部分构成。\n",
    "- stop_words：string {'english'}, list, or None (default)，停用词，可以用列表导入自己的停用词\n",
    "- lowercase：将英文全部小写，默认是True\n",
    "- max_df：float in range [0.0, 1.0] or int, default=1.0，表示得到的词/句子部分出现在文档中的最大次数，如果大于该次数，则会去掉该词/句子部分，例如，若设置为0-1之间的浮点数0.6，表示所提取的特征出现在60%以下的文档中，如果大于60%，则会从特征中删除。如果为整数X，表示该特征（很多时候是词或者句子）出现的文档数必须不大于X，否则也会删除。\n",
    "- min_df：float in range [0.0, 1.0] or int, default=1，同理max_df，只不过是设置的下阈值，表示该特征出现的文档数小于该值则会被删除。\n",
    "- vocabulary：Mapping or iterable, optional，可以用字典，例如{\"华为\":0, \"小米\":1,\"ov\":2}，其中键值keys表示要关注的词/句子等特征，values值表示该值在特征矩阵中的索引；用于传入需要重点关注的词/句子等特征。不为空None时，max_df 和 min_df参数会失效。\n",
    "- use_idf：表示是否使用idf，也就是逆文档词频方法，默认是True\n",
    "- smooth_idf：表示在计算 idf 的时候，为了防止出现除以0的错误，会在公式中加上1。\n",
    "\n",
    "### 2.3 属性和方法\n",
    "**属性**\n",
    "- vocabulary_：词汇表；字典型\n",
    "- fixed_vocabulary_：bool，如果用户提供了一个固定的词汇到索引的映射，则为真。\n",
    "- idf_：逆文档频率(IDF)向量；只有当use_idf为True时才会定义\n",
    "- stop_words_：返回停用词表\n",
    "**方法**\n",
    "- fit(raw_documents[, y])：训练数据集的词汇和IDF\n",
    "- fit_transform(raw_documents[, y])：训练数据集的词汇和IDF，返回Tf-idf-权重文本术语矩阵\n",
    "- get_feature_names()：返回统计词列表\n",
    "- get_stop_words()：返回停用词列表\n",
    "- inverse_transform(X)：返回X中非零条目的对应文本。\n",
    "- transform(raw_documents[, copy])：文本转化成Tf-idf-权重文本术语矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['are',\n",
       " 'beautiful',\n",
       " 'flowers',\n",
       " 'is',\n",
       " 'like',\n",
       " 'name',\n",
       " 'of',\n",
       " 'rose',\n",
       " 'the',\n",
       " 'these',\n",
       " 'they',\n",
       " 'very',\n",
       " 'you']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    " \n",
    "train_document = [\"The flowers are beautiful.\",\"The name of these flowers is rose, they are very beautiful.\", \"Rose is beautiful\", \"Are you like these flowers?\"]  \n",
    "test_document = [\"The flowers are mine.\", \"My flowers are beautiful\"]               \n",
    " \n",
    "# 利用函数获取文档的TFIDF值   \n",
    "transformer = TfidfVectorizer()\n",
    "X_train = transformer.fit_transform(train_document)\n",
    "X_test = transformer.transform(test_document)\n",
    " \n",
    "# 统计词列表\n",
    "word_list = transformer.get_feature_names()  # 所有统计的词\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 5, 'flowers': 2, 'are': 0, 'mine': 3, 'my': 4, 'beautiful': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 统计词字典形式\n",
    "transformer.fit(test_document).vocabulary_\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.47006328, 0.47006328, 0.47006328, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.58062167, 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.22989431, 0.22989431, 0.22989431, 0.28396521, 0.        ,\n",
       "         0.36017386, 0.36017386, 0.28396521, 0.28396521, 0.28396521,\n",
       "         0.36017386, 0.36017386, 0.        ],\n",
       "        [0.        , 0.49681612, 0.        , 0.61366674, 0.        ,\n",
       "         0.        , 0.        , 0.61366674, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.34432086, 0.        , 0.34432086, 0.        , 0.53944516,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.42530476,\n",
       "         0.        , 0.        , 0.53944516]]),\n",
       " array([[0.53256952, 0.        , 0.53256952, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.65782931, 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.57735027, 0.57735027, 0.57735027, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TFIDF权重\n",
    "weight_train = X_train.toarray()\n",
    "weight_test = X_test.toarray()\n",
    "weight_train, weight_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.22314355, 1.22314355, 1.22314355, 1.51082562, 1.91629073,\n",
       "       1.91629073, 1.91629073, 1.51082562, 1.51082562, 1.51082562,\n",
       "       1.91629073, 1.91629073, 1.91629073])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看逆文档率（IDF）\n",
    "transformer.fit(train_document).idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['are', 'beautiful', 'flowers', 'is'], dtype='<U9')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.inverse_transform(train_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
