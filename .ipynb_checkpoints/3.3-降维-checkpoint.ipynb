{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 为什么要降维？\n",
    "为了解决特征矩阵过大，导致计算量过大出现维度灾难，训练时间长等问题，需要降低维度来简化机器学习模型的训练和预测\n",
    "## 2.常用的降维方法\n",
    "### 2.1 PCA\n",
    "PCA(主成分分析)，通过正交变换将一组存在相关性的变量转换为一组线性不相关的变量，即把多指标转化为少数几个综合指标，转换后的这组变量就叫做主成分；其中每个主成分都能够反映原是变量的大部分信息，且所含信息互不重复。\n",
    "\n",
    "![](img/pca-1.png)\n",
    "\n",
    "PCA具有如下性质：\n",
    "1. 保留方差是最大的\n",
    "2. 最终的重构误差（从变换后回到原始情况）是最小的\n",
    "\n",
    "下图的例子，需要将2D降为1D，选择不同的投影线得到的结果不同，第1个投影以后方差最大，第3个方差最小，方差越大，能保留的信息更多。\n",
    "![](img/pca-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 步骤\n",
    "设有 $m$ 条 $n$ 维的数据\n",
    "1. 将原始数据按列组成 $m$ 行 $n$ 列矩阵 $X$；\n",
    "2. 将 $X$ 的每一行(代表一维特征)进行零均值化，即减去这一行的均值；\n",
    "3. 求出协方差矩阵 $C = \\frac{XX^T}{m-1}$；\n",
    "4. 求出协方差矩阵的特征值和对应的特征向量；\n",
    "5. 将特征向量按照对应的特征值大小从大到小按行排列成矩阵，取前 $k$ 行组成矩阵 $P$；\n",
    "6. $Y=PX$ 即为降维到 $k$ 维后的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 API\n",
    "sklearn.decomposition.PCA(n_components=None, *, copy=True, whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto', random_state=None)\n",
    "\n",
    "**常用参数：**\n",
    "- n_components: int, float, None 或 string，PCA算法中所要保留的主成分个数，也即保留下来的特征个数，如果 n_components = 1，将把原始数据降到一维；如果赋值为string，如n_components='mle'，将自动选取特征个数，使得满足所要求的方差百分比；如果没有赋值，默认为None，特征个数不会改变（特征数据本身会改变）。\n",
    "- copy：True 或False，默认为True，即是否需要将原始训练数据复制。\n",
    "- whiten：True 或False，默认为False，即是否白化，使得每个特征具有相同的方差。\n",
    "- svd_solver：即指定奇异值分解SVD的方法，由于特征分解是奇异值分解SVD的一个特例，一般的PCA库都是基于SVD实现的。有4个可以选择的值：{‘auto’, ‘full’, ‘arpack’, ‘randomized’}。randomized一般适用于数据量大，数据维度多同时主成分数目比例又较低的PCA降维，它使用了一些加快SVD的随机算法。 full则是传统意义上的SVD，使用了scipy库对应的实现。arpack和randomized的适用场景类似，区别是randomized使用的是scikit-learn自己的SVD实现，而arpack直接使用了scipy库的sparse SVD实现。默认是auto，即PCA类会自己去在前面讲到的三种算法里面去权衡，选择一个合适的SVD算法来降维。一般来说，使用默认值就够了。\n",
    "\n",
    "#### 2.1.3 属性和方法\n",
    "**属性：**\n",
    "- explained_variance_ratio_：返回所保留各个特征的方差百分比，如果n_components没有赋值，则所有特征都会返回一个数值且解释方差之和等于1。\n",
    "- n_components_：返回所保留的特征个数。\n",
    "\n",
    "**方法：**\n",
    "- fit(X): 用数据X来训练PCA模型。\n",
    "- fit_transform(X)：用X来训练PCA模型，同时返回降维后的数据。\n",
    "- inverse_transform(newData) ：将降维后的数据转换成原始数据，但可能不会完全一样，会有些许差别。\n",
    "- transform(X)：将数据X转换成降维后的数据，当模型训练好后，对于新输入的数据，也可以用transform方法来降维。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1, -1,  2],\n",
       "        [-2, -1,  4],\n",
       "        [-3, -2,  3]]),\n",
       " array([[ 1.44376551e+00, -1.63255223e-01,  7.08267421e-17],\n",
       "        [-5.11452038e-01,  9.21698391e-01,  7.08267421e-17],\n",
       "        [-9.32313470e-01, -7.58443167e-01,  7.08267421e-17]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "X = np.array([[-1, -1, 2], [-2, -1, 4], [-3, -2, 3]])\n",
    "pca = PCA(n_components=3)\n",
    "newX = pca.fit_transform(X)\n",
    "X, newX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.88982237e-01, 3.11017763e-01, 3.22484619e-33])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.44376551],\n",
       "       [-0.51145204],\n",
       "       [-0.93231347]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以看出第一个特征99.24%表达整个数据集，因此我们可以降到1维：\n",
    "pca = PCA(n_components=1)\n",
    "newX = pca.fit_transform(X)\n",
    "newX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 SVD 奇异值分解\n",
    "SVD 是对矩阵进行分解，SVD并不要求要分解的矩阵为方阵。假设我们的矩阵A是一个m×n的矩阵，那么我们定义矩阵 $A$ 的SVD为：\n",
    "$$ A=U \\Sigma V^T$$\n",
    "其中 $U$ 和是一个 $m \\times m$ 的方阵， $\\Sigma$ 是一个 $m \\times n$ 的矩阵，除了主对角线上的元素以外全为 0，主对角线上的每个元素都称为奇异值， $V$ 是一个 $n \\times n$ 的方阵。 $U$ 和 $V$ 都是酉矩阵，即满足\n",
    "$$U^TU=I=V^TV$$\n",
    "![](img/svd.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 API\n",
    "sklearn.decomposition.TruncatedSVD(n_components=2, *, algorithm='randomized', n_iter=5, random_state=None, tol=0.0)\n",
    "通过截断奇异值分解(SVD)进行线性维度降低。与PCA不同的是，在计算奇异值分解之前不对数据进行居中处理。这意味着它可以有效地处理稀疏矩阵。\n",
    "\n",
    "支持两种算法：一种是快速随机化SVD求解器，另一种是使用 ARPACK 作为求解器\n",
    "\n",
    "特别是，截断的SVD可以工作在由sklearn.feature_extraction.text中的向量器返回的term count/tf-idf矩阵上。在这种情况下，它被称为潜在语义分析（LSA）。\n",
    "\n",
    "**常用参数：**\n",
    "- n_components：int, default = 2，目标输出维度，必须少于特征维数，默认数据是为了可视化\n",
    "- algorithm：string, default = “randomized”，SVD求解用，arpack或者randomized\n",
    "- n_iter：int，randomized SVD solver的迭代次数，ARPACK不可用\n",
    "- random_state：default = None，随机数发生器种子\n",
    "- tol：float，ARPACK的公差\n",
    "\n",
    "#### 2.2.2 属性和方法\n",
    "**属性：**\n",
    "- components_：返回所保留的特征\n",
    "- explained_variance_：公差\n",
    "- explained_variance_ratio_：返回所保留各个特征的方差百分比\n",
    "- singular_values_：每个选定components的奇异值\n",
    "\n",
    "**方法：**\n",
    "- fit(self, X[, y])：通过X拟合LSI\n",
    "- fit_transform(self, X[, y])：用LSI模型拟合X，并对X进行降维\n",
    "- inverse_transform(self, X)：将X变换回原来的空间\n",
    "- transform(self, X)：降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1, -1],\n",
       "        [-2, -1],\n",
       "        [-3, -2],\n",
       "        [ 1,  1],\n",
       "        [ 2,  1],\n",
       "        [ 3,  2]]),\n",
       " array([[ 1.38340578],\n",
       "        [ 2.22189802],\n",
       "        [ 3.6053038 ],\n",
       "        [-1.38340578],\n",
       "        [-2.22189802],\n",
       "        [-3.6053038 ]]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "svd = TruncatedSVD(n_components=1)\n",
    "newX = svd.fit_transform(X)\n",
    "X, newX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.61628593])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99244289])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.83849224, -0.54491354]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
